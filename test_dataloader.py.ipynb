{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95761c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sample code in pytorch example combines dataloader and datasets in the same part.\n",
    "# whereas in asteroid x-umx code, they are separated, \n",
    "# it is good to separated when there are more than one dataset are trained on.\n",
    "\n",
    "from pathlib import Path\n",
    "import torch.utils.data\n",
    "import random\n",
    "import torch\n",
    "import tqdm\n",
    "import soundfile as sf\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "# import musedb\n",
    "\n",
    "class MS_21Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"MS_21 music separation dataset\n",
    "\n",
    "    The dataset consists of 150 full lengths music tracks (~10h duration) of\n",
    "    different genres along with their raw multitracks:\n",
    "    \n",
    "\n",
    "    This dataset asssumes music raw multi-tracks in (sub)folders where each folder\n",
    "    has a various number of sources. \n",
    "    A linear mix is performed on the fly by summing up the sources according to \n",
    "    the grouping information in the .csv file.\n",
    "    In order to be compatible to MUSDB_18 dataset, one can utilize the grouping information\n",
    "    to generate the traditional four stems:\n",
    "        'drums', 'vocals', 'bass', 'other'\n",
    "    \n",
    "\n",
    "    Folder Structure:\n",
    "        >>> #train/1/lead_vocals.wav ------------|\n",
    "        >>> #train/1/backing_vocals.wav ---------|\n",
    "        >>> #train/1/drums.wav ---------------+--> input (mix),\n",
    "        >>> #train/1/bass.wav -------------------|\n",
    "        >>> #train/1/accordin.wav ---------------|\n",
    "        >>> #train/1/bell.wav -------------------/\n",
    "\n",
    "        >>> #train/1/lead_vocals.wav ------------> output[target]\n",
    "\n",
    "    Args:\n",
    "        root (str): Root path of dataset\n",
    "        sources (:obj:`list` of :obj:`str`, optional): List of source names\n",
    "            that composes the mixture.\n",
    "            Defaults to MUSDB18 4 stem scenario: `vocals`, `drums`, `bass`, `other`.\n",
    "        targets (list or None, optional): List of source names to be used as\n",
    "            targets. If None, a dict with the 4 stems is returned.\n",
    "             If e.g [`vocals`, `drums`], a tensor with stacked `vocals` and\n",
    "             `drums` is returned instead of a dict. Defaults to None.\n",
    "        suffix (str, optional): Filename suffix, defaults to `.wav`.\n",
    "        split (str, optional): Dataset subfolder, defaults to `train`.\n",
    "        subset (:obj:`list` of :obj:`str`, optional): Selects a specific of\n",
    "            list of tracks to be loaded, defaults to `None` (loads all tracks).\n",
    "        segment (float, optional): Duration of segments in seconds,\n",
    "            defaults to ``None`` which loads the full-length audio tracks.\n",
    "        samples_per_track (int, optional):\n",
    "            Number of samples yielded from each track, can be used to increase\n",
    "            dataset size, defaults to `1`.\n",
    "        random_segments (boolean, optional): Enables random offset for track segments.\n",
    "        random_track_mix boolean: enables mixing of random sources from\n",
    "            different tracks to assemble mix.\n",
    "        source_augmentations (:obj:`list` of :obj:`callable`): list of augmentation\n",
    "            function names, defaults to no-op augmentations (input = output)\n",
    "        sample_rate (int, optional): Samplerate of files in dataset.\n",
    "\n",
    "    Attributes:\n",
    "        root (str): Root path of dataset\n",
    "        sources (:obj:`list` of :obj:`str`, optional): List of source names.\n",
    "            Defaults to MUSDB18 4 stem scenario: `vocals`, `drums`, `bass`, `other`.\n",
    "        suffix (str, optional): Filename suffix, defaults to `.wav`.\n",
    "        split (str, optional): Dataset subfolder, defaults to `train`.\n",
    "        subset (:obj:`list` of :obj:`str`, optional): Selects a specific of\n",
    "            list of tracks to be loaded, defaults to `None` (loads all tracks).\n",
    "        segment (float, optional): Duration of segments in seconds,\n",
    "            defaults to ``None`` which loads the full-length audio tracks.\n",
    "        samples_per_track (int, optional):\n",
    "            Number of samples yielded from each track, can be used to increase\n",
    "            dataset size, defaults to `1`.\n",
    "        random_segments (boolean, optional): Enables random offset for track segments.\n",
    "        random_track_mix boolean: enables mixing of random sources from\n",
    "            different tracks to assemble mix.\n",
    "        source_augmentations (:obj:`list` of :obj:`callable`): list of augmentation\n",
    "            function names, defaults to no-op augmentations (input = output)\n",
    "        sample_rate (int, optional): Samplerate of files in dataset.\n",
    "        tracks (:obj:`list` of :obj:`Dict`): List of track metadata\n",
    "\n",
    "    References\n",
    "        \"The 2018 Signal Separation Evaluation Campaign\" Stoter et al. 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name = \"MS_21\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        csv_file_path,\n",
    "        grouping_info = {'percussion':['Drum_Kick','Drum_Snare','Drum_HiHat','Drum_Cymbals','Drum_Overheads','Drum_Tom','Drum_Room','Percussion'\n",
    "],'vocals':['Lead_Vocal','Backing_Vocal'],'bass':'Bass','other':['Acoustic_Guitar','Electric_Guitar','Piano','Electric_Piano','Brass','String','WoodWind','Other'\n",
    "]}, # default traditional four stems grouping style\n",
    "        sources=[\"vocals\", \"bass\", \"drums\", \"other\"],\n",
    "        targets=None,\n",
    "        suffix=\".wav\",\n",
    "        split=\"\",\n",
    "        subset=None,\n",
    "        segment=None,\n",
    "        samples_per_track=1,\n",
    "        random_segments=False,\n",
    "        random_track_mix=False,\n",
    "        source_augmentations=lambda audio: audio,\n",
    "        sample_rate=44100,\n",
    "    ):\n",
    "\n",
    "        self.root = Path(root).expanduser()\n",
    "        self.csv_info = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        self.grouping_info = grouping_info\n",
    "        self.split = split\n",
    "        self.sample_rate = sample_rate\n",
    "        self.segment = segment\n",
    "        self.random_track_mix = random_track_mix\n",
    "        self.random_segments = random_segments\n",
    "        self.source_augmentations = source_augmentations\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "        self.suffix = suffix\n",
    "        self.subset = subset\n",
    "        self.samples_per_track = samples_per_track\n",
    "        self.tracks = list(self.get_tracks())\n",
    "        #print(self.tracks)\n",
    "        if not self.tracks:\n",
    "            raise RuntimeError(\"No tracks found.\")\n",
    "        # self.__getitem__(index = 1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # create a dict for storing stem grouping rule\n",
    "        \n",
    "        \n",
    "        \n",
    "        # assemble the mixture of target and interferers\n",
    "        audio_sources = {}\n",
    "\n",
    "        # get track_id\n",
    "        track_id = index // self.samples_per_track\n",
    "        print(\"track_ID = \",track_id)\n",
    "        \n",
    "        \n",
    "        if self.random_segments:\n",
    "            start = random.uniform(0, self.tracks[track_id][\"min_duration\"] - self.segment)\n",
    "        else:\n",
    "            start = 0\n",
    "\n",
    "        # create sources based on multitracks\n",
    "        for source in self.sources:\n",
    "            # optionally select a random track for each source\n",
    "            if self.random_track_mix:\n",
    "                # load a different track\n",
    "                track_id = random.choice(range(len(self.tracks)))\n",
    "                if self.random_segments:\n",
    "                    start = random.uniform(0, self.tracks[track_id][\"min_duration\"] - self.segment)\n",
    "\n",
    "            # loads the full track duration\n",
    "            start_sample = int(start * self.sample_rate)\n",
    "            # check if dur is none\n",
    "            if self.segment:\n",
    "                # stop in soundfile is calc in samples, not seconds\n",
    "                stop_sample = start_sample + int(self.segment * self.sample_rate)\n",
    "            else:\n",
    "                # set to None for reading complete file\n",
    "                stop_sample = None\n",
    "\n",
    "            # load actual audio\n",
    "#             audio, _ = sf.read(\n",
    "#                 Path(self.tracks[track_id][\"path\"] / source).with_suffix(self.suffix),\n",
    "#                 always_2d=True,\n",
    "#                 start=start_sample,\n",
    "#                 stop=stop_sample,\n",
    "#             )\n",
    "            # load multitracks and be ready to do linear mix\n",
    "            for i in self.grouping_info:\n",
    "                print(i) # get source names\n",
    "                stem_tracks = []\n",
    "                # get all instrument name within one stem\n",
    "                for j in self.grouping_info[i]:\n",
    "                    print(j)\n",
    "                    # get all multitrack filename within one instrument\n",
    "                    for m in j:\n",
    "                        print(m)\n",
    "                        stem_tracks.append(self.csv_info.iloc[track_id][m])\n",
    "                    \n",
    "                # apply linear mix within one source (stem) later can intergrate with data augmentation\n",
    "                # first load one multitrack\n",
    "                source_multitrack = {}\n",
    "                for k in stem_tracks:\n",
    "                    audio,_ = sf.read(\n",
    "                        Path(self.tracks[track_id]['path'] / k),\n",
    "                    always_2d=True,\n",
    "                    start=start_sample,\n",
    "                    stop=stop_sample,\n",
    "                    )\n",
    "                    # convert to torch tensor\n",
    "                    audio = torch.tensor(audio.T, dtype=torch.float)\n",
    "\n",
    "                    # apply multitrack-wise augmentations\n",
    "                    # audio = self.multitrack_augmentation(audio)\n",
    "                    source_multitrack[k] = audio\n",
    "                   \n",
    "                # apply linear mix over all multitracks within one source index=0\n",
    "                source_mix = torch.stack(list(source_multitrack.values())).sum(0)\n",
    "                audio_sources[i] = source_mix\n",
    "                # apply source-wise augmentations\n",
    "                # source_mix = self.source_augmentations(source_mix)\n",
    "            \n",
    "            audio_mix = torch.stack(list(audio_sources.values())).sum(0)\n",
    "            if self.targets:\n",
    "                audio_sources = torch.stack(\n",
    "                    [wav for src, wav in audio_sources.items() if src in self.targets], dim=0\n",
    "            )\n",
    "        # audio_mix a mixture over the sources, audio_sources is a concatenation of all sources\n",
    "        return audio_mix, audio_sources\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tracks) * self.samples_per_track\n",
    "\n",
    "    def get_tracks(self):\n",
    "        \"\"\"Loads input and output tracks\"\"\"\n",
    "        \"\"\"load tracks that contain all the required sources tracks\"\"\"\n",
    "        p = Path(self.root, self.split) # train and test folder\n",
    "        # p = Path(self.root)\n",
    "        \n",
    "        for track_path in tqdm.tqdm(p.iterdir()):\n",
    "            #print(track_path)\n",
    "            if track_path.is_dir():\n",
    "                if self.subset and track_path.stem not in self.subset:\n",
    "                    # skip this track\n",
    "                    continue\n",
    "\n",
    "                \n",
    "                # source_paths = [track_path / (s + self.suffix) for s in self.sources] # 固定命名\n",
    "                \n",
    "                multitrack_paths = []\n",
    "                for s in os.listdir(track_path):\n",
    "                    if s.split('.')[-1]=='wav' and s.split('.')[0]!='':\n",
    "                        multitrack_paths.append(track_path / s )\n",
    "                #print(len(multitrack_paths))\n",
    "                # 改成先读取所有wav文件，返回所有path\n",
    "                # 然后通过csv文件进行linear mix生成sources,直接读成tensor\n",
    "                if not all(sp.exists() for sp in multitrack_paths):\n",
    "                    print(\"Exclude track due to non-existing source\", track_path)\n",
    "                    continue\n",
    "\n",
    "                # get metadata\n",
    "                infos = list(map(sf.info, multitrack_paths))\n",
    "                if not all(i.samplerate == self.sample_rate for i in infos):\n",
    "                    print(\"Exclude track due to different sample rate \", track_path)\n",
    "                    continue\n",
    "\n",
    "                if self.segment is not None:\n",
    "                    # get minimum duration of track\n",
    "                    min_duration = min(i.duration for i in infos)\n",
    "                    if min_duration > self.segment:\n",
    "                        yield ({\"path\": track_path, \"min_duration\": min_duration})\n",
    "                else:\n",
    "                    yield ({\"path\": track_path, \"min_duration\": None})\n",
    "\n",
    "    def get_infos(self):\n",
    "        \"\"\"Get dataset infos (for publishing models).\n",
    "\n",
    "        Returns:\n",
    "            dict, dataset infos with keys `dataset`, `task` and `licences`.\n",
    "        \"\"\"\n",
    "        infos = dict()\n",
    "        infos[\"dataset\"] = self.dataset_name\n",
    "        infos[\"task\"] = \"enhancement\"\n",
    "        infos[\"licenses\"] = [musdb_license]\n",
    "        return infos\n",
    "\n",
    "\n",
    "musdb_license = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "41aa3e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\smc_master_thesis_2021\\asteroid\\egs\\musdb18\\X-UMX\\hierarchy.json\") as json_file:\n",
    "    data = json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "779127b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c0d39607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hie(target, d: MutableMapping, sep='.') -> MutableMapping:\n",
    "\n",
    "    # retrive the instruments names which construct the corresponding level\n",
    "    # e.g. child_level: Lead_Vocal; parent_level(vocal): Lead_Vocal, Backing_Vocal; Grandparent_level(vocal, wind):Lead_Vocal, Backing_Vocal,Woodwind, Brass\n",
    "    [flat_dict] = pd.json_normalize(d, sep=sep).to_dict(orient='records')\n",
    "    hie_fla = flat_dict\n",
    "    child_level = [target]\n",
    "    parent_level = []\n",
    "    grandparent_level = []\n",
    "    for keys,values in enumerate(hie_fla):\n",
    "        \n",
    "        if target in hie_fla[values][:]: # search every leaf node (child level)\n",
    "            print(keys,values)\n",
    "            \n",
    "            grandparent_level_name = values.split('.')[-2]\n",
    "            grandparent_level =list( d['mix'][grandparent_level_name].values())\n",
    "            grandparent_level = [item for sublist in grandparent_level for item in sublist]\n",
    "            parent_level_name = values.split('.')[-1]\n",
    "            parent_level = hie_fla[values][:]\n",
    "            \n",
    "\n",
    "    return child_level, parent_level, grandparent_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2ba14cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 mix.superlevel_wind_vocal.vocal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Lead_Vocal'],\n",
       " ['Lead_Vocal', 'Backing_Vocal'],\n",
       " ['Lead_Vocal', 'Backing_Vocal', 'Brass', 'Woodwind'])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_hie('Lead_Vocal',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b75da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 667.35it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MS_21Dataset( \n",
    "    csv_file_path = 'D:\\smc_master_thesis_2021\\MTG_2021_MASTER_THESIS\\mixing_secret_dataset_modified.csv',\n",
    "    targets = 'vocals',\n",
    "    root = 'E:/unzip_multitrack'\n",
    "    # root = 'E:/musdb18_hq'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a31426bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HUICHE~1\\AppData\\Local\\Temp/ipykernel_5324/698185813.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Drum_Kick'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset.csv_info.iloc[0]['Drum_Kick']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f3de25",
   "metadata": {},
   "source": [
    "# Original musdb18_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd5e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch.utils.data\n",
    "import random\n",
    "import torch\n",
    "import tqdm\n",
    "import soundfile as sf\n",
    "# import musedb\n",
    "\n",
    "class MUSDB18Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"MUSDB18 music separation dataset\n",
    "\n",
    "    The dataset consists of 150 full lengths music tracks (~10h duration) of\n",
    "    different genres along with their isolated stems:\n",
    "        `drums`, `bass`, `vocals` and `others`.\n",
    "\n",
    "    Out-of-the-box, asteroid does only support MUSDB18-HQ which comes as\n",
    "    uncompressed WAV files. To use the MUSDB18, please convert it to WAV first:\n",
    "\n",
    "    - MUSDB18 HQ: https://zenodo.org/record/3338373\n",
    "    - MUSDB18     https://zenodo.org/record/1117372\n",
    "\n",
    "    .. note::\n",
    "        The datasets are hosted on Zenodo and require that users\n",
    "        request access, since the tracks can only be used for academic purposes.\n",
    "        We manually check this requests.\n",
    "\n",
    "    This dataset asssumes music tracks in (sub)folders where each folder\n",
    "    has a fixed number of sources (defaults to 4). For each track, a list\n",
    "    of `sources` and a common `suffix` can be specified.\n",
    "    A linear mix is performed on the fly by summing up the sources\n",
    "\n",
    "    Due to the fact that all tracks comprise the exact same set\n",
    "    of sources, random track mixing can be used can be used,\n",
    "    where sources from different tracks are mixed together.\n",
    "\n",
    "    Folder Structure:\n",
    "        >>> #train/1/vocals.wav ---------|\n",
    "        >>> #train/1/drums.wav ----------+--> input (mix), output[target]\n",
    "        >>> #train/1/bass.wav -----------|\n",
    "        >>> #train/1/other.wav ---------/\n",
    "\n",
    "    Args:\n",
    "        root (str): Root path of dataset\n",
    "        sources (:obj:`list` of :obj:`str`, optional): List of source names\n",
    "            that composes the mixture.\n",
    "            Defaults to MUSDB18 4 stem scenario: `vocals`, `drums`, `bass`, `other`.\n",
    "        targets (list or None, optional): List of source names to be used as\n",
    "            targets. If None, a dict with the 4 stems is returned.\n",
    "             If e.g [`vocals`, `drums`], a tensor with stacked `vocals` and\n",
    "             `drums` is returned instead of a dict. Defaults to None.\n",
    "        suffix (str, optional): Filename suffix, defaults to `.wav`.\n",
    "        split (str, optional): Dataset subfolder, defaults to `train`.\n",
    "        subset (:obj:`list` of :obj:`str`, optional): Selects a specific of\n",
    "            list of tracks to be loaded, defaults to `None` (loads all tracks).\n",
    "        segment (float, optional): Duration of segments in seconds,\n",
    "            defaults to ``None`` which loads the full-length audio tracks.\n",
    "        samples_per_track (int, optional):\n",
    "            Number of samples yielded from each track, can be used to increase\n",
    "            dataset size, defaults to `1`.\n",
    "        random_segments (boolean, optional): Enables random offset for track segments.\n",
    "        random_track_mix boolean: enables mixing of random sources from\n",
    "            different tracks to assemble mix.\n",
    "        source_augmentations (:obj:`list` of :obj:`callable`): list of augmentation\n",
    "            function names, defaults to no-op augmentations (input = output)\n",
    "        sample_rate (int, optional): Samplerate of files in dataset.\n",
    "\n",
    "    Attributes:\n",
    "        root (str): Root path of dataset\n",
    "        sources (:obj:`list` of :obj:`str`, optional): List of source names.\n",
    "            Defaults to MUSDB18 4 stem scenario: `vocals`, `drums`, `bass`, `other`.\n",
    "        suffix (str, optional): Filename suffix, defaults to `.wav`.\n",
    "        split (str, optional): Dataset subfolder, defaults to `train`.\n",
    "        subset (:obj:`list` of :obj:`str`, optional): Selects a specific of\n",
    "            list of tracks to be loaded, defaults to `None` (loads all tracks).\n",
    "        segment (float, optional): Duration of segments in seconds,\n",
    "            defaults to ``None`` which loads the full-length audio tracks.\n",
    "        samples_per_track (int, optional):\n",
    "            Number of samples yielded from each track, can be used to increase\n",
    "            dataset size, defaults to `1`.\n",
    "        random_segments (boolean, optional): Enables random offset for track segments.\n",
    "        random_track_mix boolean: enables mixing of random sources from\n",
    "            different tracks to assemble mix.\n",
    "        source_augmentations (:obj:`list` of :obj:`callable`): list of augmentation\n",
    "            function names, defaults to no-op augmentations (input = output)\n",
    "        sample_rate (int, optional): Samplerate of files in dataset.\n",
    "        tracks (:obj:`list` of :obj:`Dict`): List of track metadata\n",
    "\n",
    "    References\n",
    "        \"The 2018 Signal Separation Evaluation Campaign\" Stoter et al. 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name = \"MUSDB18\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        sources=[\"vocals\", \"bass\", \"drums\", \"other\"],\n",
    "        targets=None,\n",
    "        suffix=\".wav\",\n",
    "        split=\"train\",\n",
    "        subset=None,\n",
    "        segment=None,\n",
    "        samples_per_track=1,\n",
    "        random_segments=False,\n",
    "        random_track_mix=False,\n",
    "        source_augmentations=lambda audio: audio,\n",
    "        sample_rate=44100,\n",
    "    ):\n",
    "\n",
    "        self.root = Path(root).expanduser()\n",
    "        self.split = split\n",
    "        self.sample_rate = sample_rate\n",
    "        self.segment = segment\n",
    "        self.random_track_mix = random_track_mix\n",
    "        self.random_segments = random_segments\n",
    "        self.source_augmentations = source_augmentations\n",
    "        self.sources = sources\n",
    "        self.targets = targets\n",
    "        self.suffix = suffix\n",
    "        self.subset = subset\n",
    "        self.samples_per_track = samples_per_track\n",
    "        self.tracks = list(self.get_tracks())\n",
    "        if not self.tracks:\n",
    "            raise RuntimeError(\"No tracks found.\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # assemble the mixture of target and interferers\n",
    "        audio_sources = {}\n",
    "\n",
    "        # get track_id\n",
    "        track_id = index // self.samples_per_track\n",
    "        if self.random_segments:\n",
    "            start = random.uniform(0, self.tracks[track_id][\"min_duration\"] - self.segment)\n",
    "        else:\n",
    "            start = 0\n",
    "\n",
    "        # load sources\n",
    "        for source in self.sources:\n",
    "            # optionally select a random track for each source\n",
    "            if self.random_track_mix:\n",
    "                # load a different track\n",
    "                track_id = random.choice(range(len(self.tracks)))\n",
    "                if self.random_segments:\n",
    "                    start = random.uniform(0, self.tracks[track_id][\"min_duration\"] - self.segment)\n",
    "\n",
    "            # loads the full track duration\n",
    "            start_sample = int(start * self.sample_rate)\n",
    "            # check if dur is none\n",
    "            if self.segment:\n",
    "                # stop in soundfile is calc in samples, not seconds\n",
    "                stop_sample = start_sample + int(self.segment * self.sample_rate)\n",
    "            else:\n",
    "                # set to None for reading complete file\n",
    "                stop_sample = None\n",
    "\n",
    "            # load actual audio\n",
    "            audio, _ = sf.read(\n",
    "                Path(self.tracks[track_id][\"path\"] / source).with_suffix(self.suffix),\n",
    "                always_2d=True,\n",
    "                start=start_sample,\n",
    "                stop=stop_sample,\n",
    "            )\n",
    "            # convert to torch tensor\n",
    "            audio = torch.tensor(audio.T, dtype=torch.float)\n",
    "            # apply source-wise augmentations\n",
    "            audio = self.source_augmentations(audio)\n",
    "            audio_sources[source] = audio\n",
    "\n",
    "        # apply linear mix over source index=0\n",
    "        audio_mix = torch.stack(list(audio_sources.values())).sum(0)\n",
    "        if self.targets:\n",
    "            audio_sources = torch.stack(\n",
    "                [wav for src, wav in audio_sources.items() if src in self.targets], dim=0\n",
    "            )\n",
    "        return audio_mix, audio_sources\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tracks) * self.samples_per_track\n",
    "\n",
    "    def get_tracks(self):\n",
    "        \"\"\"Loads input and output tracks\"\"\"\n",
    "        p = Path(self.root, self.split)\n",
    "        \n",
    "        for track_path in tqdm.tqdm(p.iterdir()):\n",
    "            if track_path.is_dir():\n",
    "                if self.subset and track_path.stem not in self.subset:\n",
    "                    # skip this track\n",
    "                    continue\n",
    "\n",
    "                source_paths = [track_path / (s + self.suffix) for s in self.sources]\n",
    "                if not all(sp.exists() for sp in source_paths):\n",
    "                    print(\"Exclude track due to non-existing source\", track_path)\n",
    "                    continue\n",
    "\n",
    "                # get metadata\n",
    "                infos = list(map(sf.info, source_paths))\n",
    "                if not all(i.samplerate == self.sample_rate for i in infos):\n",
    "                    print(\"Exclude track due to different sample rate \", track_path)\n",
    "                    continue\n",
    "\n",
    "                if self.segment is not None:\n",
    "                    # get minimum duration of track\n",
    "                    min_duration = min(i.duration for i in infos)\n",
    "                    if min_duration > self.segment:\n",
    "                        yield ({\"path\": track_path, \"min_duration\": min_duration})\n",
    "                else:\n",
    "                    yield ({\"path\": track_path, \"min_duration\": None})\n",
    "\n",
    "    def get_infos(self):\n",
    "        \"\"\"Get dataset infos (for publishing models).\n",
    "\n",
    "        Returns:\n",
    "            dict, dataset infos with keys `dataset`, `task` and `licences`.\n",
    "        \"\"\"\n",
    "        infos = dict()\n",
    "        infos[\"dataset\"] = self.dataset_name\n",
    "        infos[\"task\"] = \"enhancement\"\n",
    "        infos[\"licenses\"] = [musdb_license]\n",
    "        return infos\n",
    "\n",
    "\n",
    "musdb_license = dict()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
